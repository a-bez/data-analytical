{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce905bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('D:\\python_projects\\sentimentAnalysis\\dipression_finder\\dataset\\depression_dataset_reddit_cleaned.csv')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd67f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # видалення пунктуації\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # подання тексту у нижньому регістрі\n",
    "    text = text.lower()\n",
    "\n",
    "    # токенізація тексту\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # видалення стоп-слів і слів корочше 3х символів\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "\n",
    "    # видалення чисел\n",
    "    tokens = [token for token in tokens if not token.isnumeric()]\n",
    "\n",
    "    # лемматизація слів\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # об'єднання токенів\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text\n",
    "\n",
    "df['processed_text'] = df['clean_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82d7aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# утворення об'єкту TF-IDF векторизатора\n",
    "vectorizer = TfidfVectorizer()\n",
    "# векторизація входного тексту\n",
    "X = vectorizer.fit_transform(df['processed_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce00fa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# визначення кількості кластерів\n",
    "k = 5\n",
    "\n",
    "# класстеризація за використанням алгоритму K-means\n",
    "kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "clusters = kmeans.fit_predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a17349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  маркування \n",
    "df['cluster'] = clusters\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4818478",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = df.to_json()\n",
    "with open('D:\\python_projects\\Dash\\data\\main_data.json', 'w') as f:\n",
    "    f.write(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315b992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# определение количества документов в клатере\n",
    "doc_counts = df['cluster'].value_counts()\n",
    "\n",
    "# преобразование в json и сохранение на диск\n",
    "df_pie_diagram = doc_counts.to_json()\n",
    "with open('D:\\python_projects\\Dash\\my_series.json', 'w') as f:\n",
    "    f.write(df_pie_diagram)\n",
    "df_pie_diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb78de50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df=df[['processed_text','cluster']]\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df60fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def sentiment_score(new_df):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    for col in new_df.columns:\n",
    "        if new_df[col].dtype == 'object':\n",
    "            new_df['sentiment_score'] = new_df[col].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "    return new_df\n",
    "\n",
    "sentiment_score(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069c80d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sentiment_df=new_df.to_json()\n",
    "with open('D:\\python_projects\\Dash\\data\\processed_sentiment_df.json', 'w') as f:\n",
    "    f.write(processed_sentiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8c0de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_cluster_keywords(df):\n",
    "    # утворення пустого датафрейму для ключових слів\n",
    "    keywords_df = pd.DataFrame(columns=['cluster', 'keywords'])\n",
    "\n",
    "    # Групування даних датафрейму за кластерами\n",
    "    grouped = new_df.groupby('cluster')\n",
    "\n",
    "    # Итерирування по групам\n",
    "    for group_name, group_df in grouped:\n",
    "        # утворення списку слів для кожного кластера\n",
    "        words = []\n",
    "\n",
    "        # ітерування по строках в середені групи\n",
    "        for row in group_df.itertuples(index=False):\n",
    "            # розбивка строк на окремі слова\n",
    "            row_words = row.processed_text.split()\n",
    "\n",
    "            # додавання кождного слова у список слів для потокового кластера\n",
    "            words.extend(row_words)\n",
    "\n",
    "        # підрахунок, частоті слів у списку\n",
    "        word_counts = Counter(words)\n",
    "\n",
    "        # Сортировка списку слів по кількості входжень в обратному порядку\n",
    "        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # утворення строки датафрейму з ключовими словами для кожного кластеру\n",
    "        row = {'cluster': group_name, 'keywords': ', '.join([w[0] for w in sorted_words])}\n",
    "\n",
    "        # додавання строки у датафрейм з ключовими словами\n",
    "        keywords_df = keywords_df.append(row, ignore_index=True)\n",
    "\n",
    "    # повернення датафрейму з ключовими словами\n",
    "    return keywords_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa37375",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_df = find_cluster_keywords(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288edfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd01d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# определяем функцию для выбора первых 5 слов\n",
    "def first_five_words(s):\n",
    "    words = s.split()\n",
    "    if len(words) >= 5:\n",
    "        return ' '.join(words[:50])\n",
    "    else:\n",
    "        return ' '.join(words)\n",
    "\n",
    "# применяем функцию к каждому элементу объекта Series\n",
    "new_dataframe = pd.DataFrame(keywords_df['keywords'].apply(first_five_words))\n",
    "new_dataframe\n",
    "# преобразование в json и сохранение на диск\n",
    "# df_gisto = new_dataframe.to_json()\n",
    "# with open('D:\\python_projects\\Dash\\data\\joind_keywords.json', 'w') as f:\n",
    "#     f.write(df_gisto)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68930f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import Tfidfvectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from operator import itemgetter\n",
    "\n",
    "def find_keywords(df, text_column, label_column, n_keywords=10):\n",
    "    # утворення об'єкту TfidfVectorizer та навчання \n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english')\n",
    "    X = vectorizer.fit_transform(df[text_column])\n",
    "\n",
    "# утворення об'єкту Наївного Байєса та навчання його нa векторах ознак і тегах класів\n",
    "clf = Multinomialne()\n",
    "\n",
    "clf.fit(x, df[label_column])\n",
    "\n",
    "# отримано словник, де ключ - теги класів, a значення - ключових слів у класі\n",
    "\n",
    "keywords = {}\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "for i, class_label in enumerate(clf.classes_):\n",
    "    top_keywords_idx = clf.coef_[i].argsort()[::-1][:n_keywords]\n",
    "    top_keywords = itemgetter(*top_keywords_idx)(feature_names)\n",
    "    keywords[class_label] = top_keywords\n",
    "\n",
    "return keywords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
